shardName=az-westeurope-c2
[ONLY_LEGACY_CODE_PATH_SUCCEED]
======= Summary =======
count=71,
total # of org=1
total # of pipeline=1
total # of policy=1
=================== DETAILS ===================
-------- ApplyPolicy RPC validation --------
count=0
# of orgs=0
# of pipelines=0
# of policies=0

OrgIds details:

PipelineIds details:

PolicyIds details:

JSON dump:
orgIds={}
pipelineIds={}
policyIds={}

-------- Disallowed cluster attributes --------
count=71
# of orgs=1
# of pipelines=1
# of policies=1
attributes=None

OrgIds details:
4569255383179310: 71

PipelineIds details:
1e2a4e33-2f14-4d22-9aea-bd04189f0c83: 71

PolicyIds details:
306312B97300034B: 71

PolicyIds details:
`cluster_source': {'count': 71, 'orgIds': {'4569255383179310'}, 'pipelineIds': {'1e2a4e33-2f14-4d22-9aea-bd04189f0c83'}, 'policyIds': {'306312B97300034B'}}

JSON dump:
orgIds={"4569255383179310": 71}
pipelineIds={"1e2a4e33-2f14-4d22-9aea-bd04189f0c83": 71}
policyIds={"306312B97300034B": 71}
attributes={"`cluster_source'": {"count": 71, "orgIds": ["4569255383179310"], "pipelineIds": ["1e2a4e33-2f14-4d22-9aea-bd04189f0c83"], "policyIds": ["306312B97300034B"]}}

-------- Uncategorized --------
count=0
# of orgs=1
# of pipelines=1
# of policies=1

OrgIds details:
4569255383179310: 1

PipelineIds details:
1e2a4e33-2f14-4d22-9aea-bd04189f0c83: 1

JSON dump:
orgIds={"4569255383179310": 1}
pipelineIds={"1e2a4e33-2f14-4d22-9aea-bd04189f0c83": 1}
logs={"4569255383179310_1e2a4e33-2f14-4d22-9aea-bd04189f0c83": [{"legacy": {"new_cluster": {"cluster_name": "dlt-execution-a451007a-38a7-47da-bd48-9b180dcd9a9b", "spark_version": "dlt:12.1-delta-pipelines-33a695c-80beb56-634c119-custom-local", "spark_conf": {"pipelines.advancedAutoscaling.enabled": "true", "pipelines.applyChangesPreviewEnabled": "true", "pipelines.autoscaling.enableAutoscalingProfile": "true", "pipelines.autoscaling.maxNumExecutors": "2", "pipelines.autoscaling.maxUnfinishedDecommissioningRatioDefaultValue": "0.0", "pipelines.autoscaling.minNumExecutors": "1", "pipelines.cloud": "Azure", "pipelines.clusterShutdown.delay": "300s", "pipelines.dbrVersion": "12.1", "pipelines.dltDebugger.enabled": "true", "pipelines.enableExpectationsOnApplyChanges": "false", "pipelines.enableTrackHistory": "false", "pipelines.enzyme.enabled": "true", "pipelines.eventLevelDataFrameCollectUsage": "warn", "pipelines.eventLevelDataFrameSaveAsTableUsage": "warn", "pipelines.eventLevelDataFrameSaveUsage": "warn", "pipelines.eventLevelDataStreamWriterStartUsage": "warn", "pipelines.eventLevelPivotUsage": "warn", "pipelines.id": "a451007a-38a7-47da-bd48-9b180dcd9a9b", "pipelines.incompatibleViewCheck.enabled": "false", "pipelines.longRunningStageDetector.enabled": "true", "pipelines.metrics.clusterResources.enabled": "true", "pipelines.metrics.clusterUtilization.enabled": "true", "pipelines.metrics.flowProgressBacklog.enabled": "true", "pipelines.updateSnapshot.enabled": "true", "spark.databricks.acceptSoakingFeatures": "false", "spark.databricks.cloudFiles.rootSchemaLocation": "abfss://lakehouse@doplatformep01st.dfs.core.windows.net/silver/autoloader", "spark.databricks.cloudFiles.sqlApi.enabled": "true", "spark.databricks.delta.optimizeWrite.enabled": "true", "spark.databricks.delta.snapshot.prefetchCommitJson.enabled": "false", "spark.databricks.driver.hardKillUnresponsive.enabled": "false", "spark.databricks.driver.ipykernel.commChannelEnabled": "false", "spark.databricks.driverNfs.enabled": "true", "spark.databricks.preemption.enabled": "false", "spark.databricks.sql.MaterializedViewReadAllowInDBR": "true", "spark.debugger.enabled": "true", "spark.debugger.eventListener": "org.apache.spark.debugger.DLTDebuggerEventReporter", "spark.decommission.enabled": "true", "spark.hadoop.fs.azure.account.auth.type.doanalyticsfilesp01st.dfs.core.windows.net": "OAuth", "spark.hadoop.fs.azure.account.auth.type.doanalyticsplatformp01st.dfs.core.windows.net": "OAuth", "spark.hadoop.fs.azure.account.auth.type.doplatformep01st.dfs.core.windows.net": "OAuth", "spark.hadoop.fs.azure.account.oauth.provider.type.doanalyticsfilesp01st.dfs.core.windows.net": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider", "spark.hadoop.fs.azure.account.oauth.provider.type.doanalyticsplatformp01st.dfs.core.windows.net": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider", "spark.hadoop.fs.azure.account.oauth.provider.type.doplatformep01st.dfs.core.windows.net": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider", "spark.hadoop.fs.azure.account.oauth2.client.endpoint.doanalyticsfilesp01st.dfs.core.windows.net": "https://login.microsoftonline.com/6b4bdb5c-ff22-4345-b681-b7515d662897/oauth2/token", "spark.hadoop.fs.azure.account.oauth2.client.endpoint.doanalyticsplatformp01st.dfs.core.windows.net": "https://login.microsoftonline.com/6b4bdb5c-ff22-4345-b681-b7515d662897/oauth2/token", "spark.hadoop.fs.azure.account.oauth2.client.endpoint.doplatformep01st.dfs.core.windows.net": "https://login.microsoftonline.com/6b4bdb5c-ff22-4345-b681-b7515d662897/oauth2/token", "spark.hadoop.fs.azure.account.oauth2.client.id.doanalyticsfilesp01st.dfs.core.windows.net": "bf434b48-3785-414d-9e4a-a346d7130d5f", "spark.hadoop.fs.azure.account.oauth2.client.id.doanalyticsplatformp01st.dfs.core.windows.net": "bf434b48-3785-414d-9e4a-a346d7130d5f", "spark.hadoop.fs.azure.account.oauth2.client.id.doplatformep01st.dfs.core.windows.net": "bf434b48-3785-414d-9e4a-a346d7130d5f", "spark.hadoop.fs.azure.account.oauth2.client.secret.doanalyticsfilesp01st.dfs.core.windows.net": "{{secrets/storage/do-analytics-ep-storage-spn}}", "spark.hadoop.fs.azure.account.oauth2.client.secret.doanalyticsplatformp01st.dfs.core.windows.net": "{{secrets/storage/do-analytics-ep-storage-spn}}", "spark.hadoop.fs.azure.account.oauth2.client.secret.doplatformep01st.dfs.core.windows.net": "{{secrets/storage/do-analytics-ep-storage-spn}}", "spark.sql.streaming.stopTimeout": "60s", "spark.storage.decommission.enabled": "true", "spark.storage.decommission.rddBlocks.enabled": "true", "spark.storage.decommission.shuffleBlocks.enabled": "true", "pipelines.clusterSpecChecksum": "6D1FED1EF6BBBFA38D7D958FAF04D253"}, "cluster_creator": "PIPELINE_LAUNCHER", "node_type_id": "Standard_DS3_v2", "driver_node_type_id": "Standard_DS4_v2", "custom_tags": {"cluster_type": "default"}, "spark_env_vars": {"PEX_ENV_FILE": "/dbfs/dodp/lakehouse/lakehouse.pex", "SCRUB_PEX_ENV_GLOB": "/root/.pex/**/dlt.py"}, "autotermination_minutes": 0, "enable_elastic_disk": true, "enable_jobs_autostart": false, "init_scripts": [{"dbfs": {"destination": "dbfs:/dodp/tools/pex_loader/init_pex_databricks.sh"}}], "policy_id": "30640179C40028B5", "enable_local_disk_encryption": false, "billing_info": {"dlt_billing_info": {"tier": "ADVANCED", "is_photon_enabled": false}}, "runtime_engine": "STANDARD", "autoscale": {"min_workers": 1, "max_workers": 2, "profile": "ENHANCED"}}}, "latest": "Unexpected error while creating and validating the execution ClusterSpec:\nTEMPORARILY_UNAVAILABLE: Service for this endpoint is temporarily unavailable"}]}


-------- Failed to parse --------
count=0
log JSON dump=[]
